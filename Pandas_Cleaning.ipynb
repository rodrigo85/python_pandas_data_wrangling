{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e0eb416-ae19-4c12-acd1-dbd963761128",
   "metadata": {},
   "source": [
    "# Load and Preview Data\n",
    "\n",
    "This cell loads the Excel file into a pandas DataFrame and shows basic information about the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaa4ef55-ceeb-48d2-ad24-b110c7ab3b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28085 entries, 0 to 28084\n",
      "Data columns (total 18 columns):\n",
      " #   Column                                                                                                                                                                                                                                Non-Null Count  Dtype         \n",
      "---  ------                                                                                                                                                                                                                                --------------  -----         \n",
      " 0   Timestamp                                                                                                                                                                                                                             28085 non-null  datetime64[ns]\n",
      " 1   How old are you?                                                                                                                                                                                                                      28085 non-null  object        \n",
      " 2   What industry do you work in?                                                                                                                                                                                                         28011 non-null  object        \n",
      " 3   Job title                                                                                                                                                                                                                             28084 non-null  object        \n",
      " 4   If your job title needs additional context, please clarify here:                                                                                                                                                                      7267 non-null   object        \n",
      " 5   What is your annual salary? (You'll indicate the currency in a later question. If you are part-time or hourly, please enter an annualized equivalent -- what you would earn if you worked the job 40 hours a week, 52 weeks a year.)  28085 non-null  int64         \n",
      " 6   How much additional monetary compensation do you get, if any (for example, bonuses or overtime in an average year)? Please only include monetary compensation here, not the value of benefits.                                        20780 non-null  float64       \n",
      " 7   Please indicate the currency                                                                                                                                                                                                          28085 non-null  object        \n",
      " 8   If \"Other,\" please indicate the currency here:                                                                                                                                                                                        207 non-null    object        \n",
      " 9   If your income needs additional context, please provide it here:                                                                                                                                                                      3044 non-null   object        \n",
      " 10  What country do you work in?                                                                                                                                                                                                          28085 non-null  object        \n",
      " 11  If you're in the U.S., what state do you work in?                                                                                                                                                                                     23059 non-null  object        \n",
      " 12  What city do you work in?                                                                                                                                                                                                             28003 non-null  object        \n",
      " 13  How many years of professional work experience do you have overall?                                                                                                                                                                   28085 non-null  object        \n",
      " 14  How many years of professional work experience do you have in your field?                                                                                                                                                             28085 non-null  object        \n",
      " 15  What is your highest level of education completed?                                                                                                                                                                                    27863 non-null  object        \n",
      " 16  What is your gender?                                                                                                                                                                                                                  27914 non-null  object        \n",
      " 17  What is your race? (Choose all that apply.)                                                                                                                                                                                           27908 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(1), int64(1), object(15)\n",
      "memory usage: 3.9+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Set pandas display options to avoid truncation and line breaks\n",
    "pd.set_option('display.max_rows', None)      # Show all rows\n",
    "pd.set_option('display.max_columns', None)   # Show all columns\n",
    "pd.set_option('display.width', 1000)         # Set a wider display width\n",
    "pd.set_option('display.colheader_justify', 'left')  # Justify column headers to the left\n",
    "\n",
    "df = pd.read_excel('./shared/Ask A Manager Salary Survey 2021 (Responses).xlsx', engine='openpyxl')\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc82c233-3258-4f2c-870d-45a653aefe0e",
   "metadata": {},
   "source": [
    "## Helper Functions to display data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26542d5b-1719-46ff-a568-62a8dbde281f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_unique_values(df, column_name):\n",
    "    \"\"\"\n",
    "    Displays the unique values of a specified column in a DataFrame, ordered alphabetically or numerically.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame containing the data.\n",
    "    column_name (str): The column name whose unique values you want to display.\n",
    "    \n",
    "    Returns:\n",
    "    None: Prints the unique values in the specified column, sorted in ascending order.\n",
    "    \"\"\"\n",
    "    if column_name in df.columns:\n",
    "        unique_values = sorted(df[column_name].unique())\n",
    "        print(f\"Unique values in '{column_name}' (sorted):\")\n",
    "        for value in unique_values:\n",
    "            print(value)\n",
    "    else:\n",
    "        print(f\"Column '{column_name}' does not exist in the DataFrame.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18f96c69-3ac0-4342-834c-a83cc9603e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_value_counts(df, column_name):\n",
    "    \"\"\"\n",
    "    Displays the count of unique values in a specified column of a DataFrame,\n",
    "    sorted in descending order.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to analyze.\n",
    "    column_name (str): The column to count unique values from.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    if column_name in df.columns:\n",
    "        value_counts = df[column_name].value_counts().sort_values(ascending=False)\n",
    "        print(f\"Unique value counts for column '{column_name}':\\n\")\n",
    "        print(value_counts)\n",
    "    else:\n",
    "        print(f\"Column '{column_name}' not found in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c6250f-a51c-4bc3-a336-256735ede57f",
   "metadata": {},
   "source": [
    "# Rename Columns for Clarity\n",
    "\n",
    "I renamed the columns in the DataFrame because the original titles from the Excel file were too long and hard to work with in code. By shortening them, I make the DataFrame easier to manipulate and reference. For instance, long titles like \"What is your job title?\" are now simply \"Job Title,\" which is cleaner and more manageable for analysis. \n",
    "This step also improves code readability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2000e839-6e26-4475-ba14-91555d259cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Age Group</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Job Title Additional Context</th>\n",
       "      <th>Annual Salary</th>\n",
       "      <th>Additional Compensation</th>\n",
       "      <th>Currency</th>\n",
       "      <th>Other Currency</th>\n",
       "      <th>Income Additional Context</th>\n",
       "      <th>Country</th>\n",
       "      <th>State</th>\n",
       "      <th>City</th>\n",
       "      <th>Total Experience</th>\n",
       "      <th>Field Experience</th>\n",
       "      <th>Education Level</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Race</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-04-27 11:02:09.743</td>\n",
       "      <td>25-34</td>\n",
       "      <td>Education (Higher Education)</td>\n",
       "      <td>Research and Instruction Librarian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>USD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>Boston</td>\n",
       "      <td>5-7 years</td>\n",
       "      <td>5-7 years</td>\n",
       "      <td>Master's degree</td>\n",
       "      <td>Woman</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-04-27 11:02:21.562</td>\n",
       "      <td>25-34</td>\n",
       "      <td>Computing or Tech</td>\n",
       "      <td>Change &amp; Internal Communications Manager</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54600</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>GBP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cambridge</td>\n",
       "      <td>8 - 10 years</td>\n",
       "      <td>5-7 years</td>\n",
       "      <td>College degree</td>\n",
       "      <td>Non-binary</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-04-27 11:02:38.125</td>\n",
       "      <td>25-34</td>\n",
       "      <td>Accounting, Banking &amp; Finance</td>\n",
       "      <td>Marketing Specialist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>USD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>Chattanooga</td>\n",
       "      <td>2 - 4 years</td>\n",
       "      <td>2 - 4 years</td>\n",
       "      <td>College degree</td>\n",
       "      <td>Woman</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-04-27 11:02:40.643</td>\n",
       "      <td>25-34</td>\n",
       "      <td>Nonprofits</td>\n",
       "      <td>Program Manager</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62000</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>USD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>USA</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>Milwaukee</td>\n",
       "      <td>8 - 10 years</td>\n",
       "      <td>5-7 years</td>\n",
       "      <td>College degree</td>\n",
       "      <td>Woman</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-04-27 11:02:41.793</td>\n",
       "      <td>25-34</td>\n",
       "      <td>Accounting, Banking &amp; Finance</td>\n",
       "      <td>Accounting Manager</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60000</td>\n",
       "      <td>7000.0</td>\n",
       "      <td>USD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>South Carolina</td>\n",
       "      <td>Greenville</td>\n",
       "      <td>8 - 10 years</td>\n",
       "      <td>5-7 years</td>\n",
       "      <td>College degree</td>\n",
       "      <td>Woman</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Timestamp               Age Group Industry                       Job Title                                 Job Title Additional Context  Annual Salary  Additional Compensation Currency Other Currency Income Additional Context Country         State           City         Total Experience Field Experience Education Level  Gender      Race  \n",
       "0 2021-04-27 11:02:09.743  25-34      Education (Higher Education)        Research and Instruction Librarian  NaN                          55000             0.0                   USD      NaN            NaN                        United States   Massachusetts       Boston     5-7 years       5-7 years      Master's degree       Woman  White\n",
       "1 2021-04-27 11:02:21.562  25-34                 Computing or Tech  Change & Internal Communications Manager  NaN                          54600          4000.0                   GBP      NaN            NaN                       United Kingdom             NaN    Cambridge  8 - 10 years       5-7 years       College degree  Non-binary  White\n",
       "2 2021-04-27 11:02:38.125  25-34     Accounting, Banking & Finance                      Marketing Specialist  NaN                          34000             NaN                   USD      NaN            NaN                                   US       Tennessee  Chattanooga   2 - 4 years     2 - 4 years       College degree       Woman  White\n",
       "3 2021-04-27 11:02:40.643  25-34                        Nonprofits                           Program Manager  NaN                          62000          3000.0                   USD      NaN            NaN                                  USA       Wisconsin    Milwaukee  8 - 10 years       5-7 years       College degree       Woman  White\n",
       "4 2021-04-27 11:02:41.793  25-34     Accounting, Banking & Finance                        Accounting Manager  NaN                          60000          7000.0                   USD      NaN            NaN                                   US  South Carolina   Greenville  8 - 10 years       5-7 years       College degree       Woman  White"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns = ['Timestamp', 'Age Group', 'Industry', 'Job Title', 'Job Title Additional Context',\n",
    "              'Annual Salary', 'Additional Compensation', \n",
    "              'Currency','Other Currency', 'Income Additional Context', 'Country', 'State', 'City', \n",
    "              'Total Experience', 'Field Experience', \n",
    "              'Education Level', 'Gender', 'Race']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340bd1d5-846a-4e67-b99f-586e49581081",
   "metadata": {},
   "source": [
    "# Analyzing Missing Values\n",
    "\n",
    "In this step, I wanted to understand the extent of missing data in the dataset. I calculated both the count and percentage of missing values per column to help me decide which columns may need further cleaning or handling before moving forward with analysis.\n",
    "\n",
    "After running the cell, I got a clearer understanding of which columns have substantial missing data, and which ones seem reliable for analysis.\n",
    "\n",
    "## Columns to Trust\n",
    "The following columns have no missing data and can be trusted for immediate use:\n",
    "- **Timestamp**\n",
    "- **Age Group**\n",
    "- **Annual Salary**\n",
    "- **Currency**\n",
    "- **Total Experience**\n",
    "- **Field Experience**\n",
    "- **Country**\n",
    "\n",
    "## Rows to drop\n",
    "For the purposes of this analysis, I will drop rows that have missing values in the **Job Title** and **Industry** columns, as these fields are crucial for understanding salary data and making meaningful comparisons.\n",
    "\n",
    "## Columns to Ignore\n",
    "Certain columns have a very high percentage of missing data, indicating that they might be less relevant for the analysis or could be safely ignored. These columns include:\n",
    "- **Job Title Additional Context**\n",
    "- **Other Currency**\n",
    "- **Income Additional Context**\n",
    "\n",
    "Since the missing values in these columns are so high, they may not provide meaningful insights and can be ignored during further analysis.\n",
    "\n",
    "## Columns to Handle Later\n",
    "Certain columns have moderate levels of missing data and should be addressed if future metrics or analyses depend on them. For now, I'll skip handling them, but this could be an enhancement for future analysis:\n",
    "- **Education Level**\n",
    "- **Gender**\n",
    "- **Race**\n",
    "- **State**\n",
    "- **City**\n",
    "\n",
    "These columns might require attention if relevant metrics are involved later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5849bd4a-8731-4474-a64c-cf7dcaaf8a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp                           0\n",
      "Age Group                           0\n",
      "Industry                           74\n",
      "Job Title                           1\n",
      "Job Title Additional Context    20818\n",
      "Annual Salary                       0\n",
      "Additional Compensation          7305\n",
      "Currency                            0\n",
      "Other Currency                  27878\n",
      "Income Additional Context       25041\n",
      "Country                             0\n",
      "State                            5026\n",
      "City                               82\n",
      "Total Experience                    0\n",
      "Field Experience                    0\n",
      "Education Level                   222\n",
      "Gender                            171\n",
      "Race                              177\n",
      "dtype: int64\n",
      "Timestamp                        0.000000\n",
      "Age Group                        0.000000\n",
      "Industry                         0.263486\n",
      "Job Title                        0.003561\n",
      "Job Title Additional Context    74.124978\n",
      "Annual Salary                    0.000000\n",
      "Additional Compensation         26.010326\n",
      "Currency                         0.000000\n",
      "Other Currency                  99.262952\n",
      "Income Additional Context       89.161474\n",
      "Country                          0.000000\n",
      "State                           17.895674\n",
      "City                             0.291971\n",
      "Total Experience                 0.000000\n",
      "Field Experience                 0.000000\n",
      "Education Level                  0.790458\n",
      "Gender                           0.608866\n",
      "Race                             0.630230\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "missing_values = df.isnull().sum()\n",
    "\n",
    "missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
    "\n",
    "print(missing_values)\n",
    "print(missing_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570f12e2-d189-46e0-ba73-62bf64120691",
   "metadata": {},
   "source": [
    "# Data Cleaning Process and Reasoning\n",
    "\n",
    "In this analysis, I performed several data cleaning steps to prepare the dataset for further exploration and ensure that it's suitable for analysis. Here’s a breakdown of what was done and the reasoning behind each step:\n",
    "\n",
    "#### Step 1: Dropping Columns with High Missing Percentages\n",
    "I dropped the columns **Job Title Additional Context**, **Income Additional Context**, and **Other Currency** because they had a very high percentage of missing values (above 70%). Keeping these columns would likely introduce noise into the analysis, and since they are not essential for the core questions being addressed, I opted to remove them.\n",
    "\n",
    "#### Step 2: Handling Missing Values in Numeric Columns\n",
    "For the **Additional Compensation** column, I filled the missing values with `0.0`. The assumption here is that if no value was provided, the respondent did not receive any additional compensation. This ensures that the analysis is not skewed by missing data.\n",
    "\n",
    "#### Step 3: Filling Missing Values in State and City Columns\n",
    "For the **State** and **City** columns, I filled the missing values with 'Unknown'. This approach prevents any data loss while still distinguishing between known and unknown values. Using 'Unknown' is a common best practice when there is a need to preserve data but the true value is unavailable.\n",
    "\n",
    "#### Step 4: Imputing Categorical Columns with the Most Frequent Value\n",
    "I imputed missing values in the **Education Level**, **Gender**, and **Race** columns using the mode (the most frequent value). This method ensures that these key categorical variables remain usable while minimizing any potential bias introduced by missing data.\n",
    "\n",
    "### Enhancement: Rather than arbitrarily choosing a placeholder for missing categorical data, imputing with the mode ensures consistency and prevents introducing outliers.\n",
    "\n",
    "#### Step 5: Final Check on Missing Values\n",
    "After all the imputations and cleaning, I performed a final check to confirm that no missing values remain in the dataset. This step ensures that the dataset is now fully prepared for analysis.\n",
    "\n",
    "#### Step 6: Final Data Structure and Initial Review\n",
    "Finally, I reviewed the structure of the cleaned dataset and displayed the first few rows to confirm that everything is in order before proceeding with any analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50af4a4a-06c8-4e8d-a803-ed4d109a21db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp                  0\n",
      "Age Group                  0\n",
      "Industry                   0\n",
      "Job Title                  0\n",
      "Annual Salary              0\n",
      "Additional Compensation    0\n",
      "Currency                   0\n",
      "Country                    0\n",
      "State                      0\n",
      "City                       0\n",
      "Total Experience           0\n",
      "Field Experience           0\n",
      "Education Level            0\n",
      "Gender                     0\n",
      "Race                       0\n",
      "dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28085 entries, 0 to 28084\n",
      "Data columns (total 15 columns):\n",
      " #   Column                   Non-Null Count  Dtype         \n",
      "---  ------                   --------------  -----         \n",
      " 0   Timestamp                28085 non-null  datetime64[ns]\n",
      " 1   Age Group                28085 non-null  object        \n",
      " 2   Industry                 28085 non-null  object        \n",
      " 3   Job Title                28085 non-null  object        \n",
      " 4   Annual Salary            28085 non-null  int64         \n",
      " 5   Additional Compensation  28085 non-null  float64       \n",
      " 6   Currency                 28085 non-null  object        \n",
      " 7   Country                  28085 non-null  object        \n",
      " 8   State                    28085 non-null  object        \n",
      " 9   City                     28085 non-null  object        \n",
      " 10  Total Experience         28085 non-null  object        \n",
      " 11  Field Experience         28085 non-null  object        \n",
      " 12  Education Level          28085 non-null  object        \n",
      " 13  Gender                   28085 non-null  object        \n",
      " 14  Race                     28085 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(1), int64(1), object(12)\n",
      "memory usage: 3.2+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Age Group</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Annual Salary</th>\n",
       "      <th>Additional Compensation</th>\n",
       "      <th>Currency</th>\n",
       "      <th>Country</th>\n",
       "      <th>State</th>\n",
       "      <th>City</th>\n",
       "      <th>Total Experience</th>\n",
       "      <th>Field Experience</th>\n",
       "      <th>Education Level</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Race</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-04-27 11:02:09.743</td>\n",
       "      <td>25-34</td>\n",
       "      <td>Education (Higher Education)</td>\n",
       "      <td>Research and Instruction Librarian</td>\n",
       "      <td>55000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>USD</td>\n",
       "      <td>United States</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>Boston</td>\n",
       "      <td>5-7 years</td>\n",
       "      <td>5-7 years</td>\n",
       "      <td>Master's degree</td>\n",
       "      <td>Woman</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-04-27 11:02:21.562</td>\n",
       "      <td>25-34</td>\n",
       "      <td>Computing or Tech</td>\n",
       "      <td>Change &amp; Internal Communications Manager</td>\n",
       "      <td>54600</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>GBP</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Cambridge</td>\n",
       "      <td>8 - 10 years</td>\n",
       "      <td>5-7 years</td>\n",
       "      <td>College degree</td>\n",
       "      <td>Non-binary</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-04-27 11:02:38.125</td>\n",
       "      <td>25-34</td>\n",
       "      <td>Accounting, Banking &amp; Finance</td>\n",
       "      <td>Marketing Specialist</td>\n",
       "      <td>34000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>USD</td>\n",
       "      <td>US</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>Chattanooga</td>\n",
       "      <td>2 - 4 years</td>\n",
       "      <td>2 - 4 years</td>\n",
       "      <td>College degree</td>\n",
       "      <td>Woman</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-04-27 11:02:40.643</td>\n",
       "      <td>25-34</td>\n",
       "      <td>Nonprofits</td>\n",
       "      <td>Program Manager</td>\n",
       "      <td>62000</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>USD</td>\n",
       "      <td>USA</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>Milwaukee</td>\n",
       "      <td>8 - 10 years</td>\n",
       "      <td>5-7 years</td>\n",
       "      <td>College degree</td>\n",
       "      <td>Woman</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-04-27 11:02:41.793</td>\n",
       "      <td>25-34</td>\n",
       "      <td>Accounting, Banking &amp; Finance</td>\n",
       "      <td>Accounting Manager</td>\n",
       "      <td>60000</td>\n",
       "      <td>7000.0</td>\n",
       "      <td>USD</td>\n",
       "      <td>US</td>\n",
       "      <td>South Carolina</td>\n",
       "      <td>Greenville</td>\n",
       "      <td>8 - 10 years</td>\n",
       "      <td>5-7 years</td>\n",
       "      <td>College degree</td>\n",
       "      <td>Woman</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Timestamp               Age Group Industry                       Job Title                                  Annual Salary  Additional Compensation Currency Country         State           City         Total Experience Field Experience Education Level  Gender      Race  \n",
       "0 2021-04-27 11:02:09.743  25-34      Education (Higher Education)        Research and Instruction Librarian  55000             0.0                   USD       United States   Massachusetts       Boston     5-7 years       5-7 years      Master's degree       Woman  White\n",
       "1 2021-04-27 11:02:21.562  25-34                 Computing or Tech  Change & Internal Communications Manager  54600          4000.0                   GBP      United Kingdom         Unknown    Cambridge  8 - 10 years       5-7 years       College degree  Non-binary  White\n",
       "2 2021-04-27 11:02:38.125  25-34     Accounting, Banking & Finance                      Marketing Specialist  34000             0.0                   USD                  US       Tennessee  Chattanooga   2 - 4 years     2 - 4 years       College degree       Woman  White\n",
       "3 2021-04-27 11:02:40.643  25-34                        Nonprofits                           Program Manager  62000          3000.0                   USD                 USA       Wisconsin    Milwaukee  8 - 10 years       5-7 years       College degree       Woman  White\n",
       "4 2021-04-27 11:02:41.793  25-34     Accounting, Banking & Finance                        Accounting Manager  60000          7000.0                   USD                  US  South Carolina   Greenville  8 - 10 years       5-7 years       College degree       Woman  White"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define columns to drop and constants for imputation\n",
    "COLUMNS_TO_DROP = ['Job Title Additional Context', 'Income Additional Context', 'Other Currency']\n",
    "NUMERIC_FILL_VALUE = 0.0\n",
    "CATEGORICAL_FILL_VALUE = 'Unknown'\n",
    "\n",
    "# Function to fill missing values for categorical columns with mode\n",
    "def fill_missing_with_mode(df, columns):\n",
    "    for col in columns:\n",
    "        df[col] = df[col].fillna(df[col].mode()[0])\n",
    "    return df\n",
    "\n",
    "# Drop columns with high missing percentage\n",
    "df_clean = df.drop(COLUMNS_TO_DROP, axis=1, inplace=False)\n",
    "# Drop columns with negative salary\n",
    "df_clean = df_clean[df_clean['Annual Salary'] >= 0]\n",
    "\n",
    "# Fill missing values for numeric columns\n",
    "df_clean['Additional Compensation'] = df_clean['Additional Compensation'].fillna(NUMERIC_FILL_VALUE)\n",
    "\n",
    "# Fill missing values for State and City with 'Unknown'\n",
    "df_clean['State'] = df_clean['State'].fillna(CATEGORICAL_FILL_VALUE)\n",
    "df_clean['City'] = df_clean['City'].fillna(CATEGORICAL_FILL_VALUE)\n",
    "\n",
    "# Impute categorical columns with the most frequent value (mode)\n",
    "categorical_columns = ['Industry', 'Job Title', 'Education Level', 'Gender', 'Race']\n",
    "df_clean = fill_missing_with_mode(df_clean, categorical_columns)\n",
    "\n",
    "# Final check on missing values after cleaning\n",
    "print(df_clean.isnull().sum())\n",
    "\n",
    "# Display the cleaned DataFrame structure and first few rows\n",
    "df_clean.info()\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d592efcf-3a23-4358-bc7a-e736a809efeb",
   "metadata": {},
   "source": [
    "# Column Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61efc070-dac6-49f2-8662-8a4e50489c5a",
   "metadata": {},
   "source": [
    "### Column Race\n",
    "\n",
    "In this step, I wanted to get an overview of how many unique values each column in the dataset contains. To do this, I wrote a simple command to display the number of distinct values per column.\n",
    "With this output, I found it interesting to see that the 'Race' column has a surprisingly high number of distinct combinations (51 unique values). I decided to explore further by printing all the unique values in this column to understand what these combinations looked like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6e2b61e-ad78-40eb-b826-2fa9447bf05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp                  28080\n",
      "Age Group                      7\n",
      "Industry                    1220\n",
      "Job Title                  14360\n",
      "Annual Salary               3668\n",
      "Additional Compensation      847\n",
      "Currency                      11\n",
      "Country                      379\n",
      "State                        136\n",
      "City                        4835\n",
      "Total Experience               8\n",
      "Field Experience               8\n",
      "Education Level                6\n",
      "Gender                         5\n",
      "Race                          51\n",
      "dtype: int64\n",
      "['White' 'Hispanic, Latino, or Spanish origin, White'\n",
      " 'Asian or Asian American, White' 'Asian or Asian American'\n",
      " 'Another option not listed here or prefer not to answer'\n",
      " 'Hispanic, Latino, or Spanish origin'\n",
      " 'Middle Eastern or Northern African'\n",
      " 'Hispanic, Latino, or Spanish origin, Middle Eastern or Northern African, White'\n",
      " 'Black or African American' 'Black or African American, White'\n",
      " 'Black or African American, Hispanic, Latino, or Spanish origin, White'\n",
      " 'Native American or Alaska Native'\n",
      " 'Native American or Alaska Native, White'\n",
      " 'Hispanic, Latino, or Spanish origin, Another option not listed here or prefer not to answer'\n",
      " 'Black or African American, Middle Eastern or Northern African, Native American or Alaska Native, White'\n",
      " 'White, Another option not listed here or prefer not to answer'\n",
      " 'Black or African American, Native American or Alaska Native, White'\n",
      " 'Asian or Asian American, Another option not listed here or prefer not to answer'\n",
      " 'Middle Eastern or Northern African, White'\n",
      " 'Asian or Asian American, Black or African American, White'\n",
      " 'Black or African American, Hispanic, Latino, or Spanish origin'\n",
      " 'Asian or Asian American, Black or African American'\n",
      " 'Asian or Asian American, Hispanic, Latino, or Spanish origin, White'\n",
      " 'Native American or Alaska Native, White, Another option not listed here or prefer not to answer'\n",
      " 'Asian or Asian American, Hispanic, Latino, or Spanish origin'\n",
      " 'Asian or Asian American, Native American or Alaska Native, White'\n",
      " 'Hispanic, Latino, or Spanish origin, Native American or Alaska Native'\n",
      " 'Black or African American, Middle Eastern or Northern African, White'\n",
      " 'Black or African American, Hispanic, Latino, or Spanish origin, Native American or Alaska Native, White'\n",
      " 'Black or African American, Another option not listed here or prefer not to answer'\n",
      " 'Native American or Alaska Native, Another option not listed here or prefer not to answer'\n",
      " 'Asian or Asian American, White, Another option not listed here or prefer not to answer'\n",
      " 'Asian or Asian American, Middle Eastern or Northern African'\n",
      " 'Asian or Asian American, Hispanic, Latino, or Spanish origin, Native American or Alaska Native, White'\n",
      " 'Hispanic, Latino, or Spanish origin, Middle Eastern or Northern African'\n",
      " 'Hispanic, Latino, or Spanish origin, Native American or Alaska Native, White'\n",
      " 'Middle Eastern or Northern African, White, Another option not listed here or prefer not to answer'\n",
      " 'Hispanic, Latino, or Spanish origin, White, Another option not listed here or prefer not to answer'\n",
      " 'Asian or Asian American, Black or African American, Hispanic, Latino, or Spanish origin'\n",
      " 'Asian or Asian American, Black or African American, Native American or Alaska Native, White'\n",
      " 'Middle Eastern or Northern African, Native American or Alaska Native, White'\n",
      " 'Asian or Asian American, Middle Eastern or Northern African, White'\n",
      " 'Black or African American, Middle Eastern or Northern African'\n",
      " 'Hispanic, Latino, or Spanish origin, Native American or Alaska Native, Another option not listed here or prefer not to answer'\n",
      " 'Asian or Asian American, Native American or Alaska Native'\n",
      " 'Middle Eastern or Northern African, Native American or Alaska Native'\n",
      " 'Asian or Asian American, Hispanic, Latino, or Spanish origin, Another option not listed here or prefer not to answer'\n",
      " 'Asian or Asian American, Hispanic, Latino, or Spanish origin, White, Another option not listed here or prefer not to answer'\n",
      " 'Asian or Asian American, Black or African American, Hispanic, Latino, or Spanish origin, Middle Eastern or Northern African, Native American or Alaska Native, White, Another option not listed here or prefer not to answer'\n",
      " 'Black or African American, Native American or Alaska Native'\n",
      " 'Asian or Asian American, Black or African American, Hispanic, Latino, or Spanish origin, Native American or Alaska Native']\n"
     ]
    }
   ],
   "source": [
    "# Display the number of distinct values in each column\n",
    "distinct_values_count = df_clean.nunique()\n",
    "\n",
    "# Display the result\n",
    "print(distinct_values_count)\n",
    "\n",
    "# Show all unique values in the 'Race' column\n",
    "unique_race_values = df_clean['Race'].unique()\n",
    "\n",
    "# Display the unique values\n",
    "print(unique_race_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fc6093-ada3-411b-965a-40ab24bf5558",
   "metadata": {},
   "source": [
    "**Comments:**\n",
    "\n",
    "While this exploration yielded a variety of combinations involving multiple racial identities and options, I realized that analyzing these combinations requires an in-depth understanding of racial identity classifications, which is not my area of expertise. Given the complexity and nuanced nature of these combinations, I've decided to skip any analysis involving the 'Race' column for now, as I don't feel confident in using it as a reliable metric for this dataset.\n",
    "\n",
    "Because of this, we won't be using 'Race' in any further analysis. This decision limits the scope of our insights regarding racial diversity, but I believe it's important to avoid misinterpreting or misrepresenting data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fd6f37-5a79-4275-857f-cf576986763c",
   "metadata": {},
   "source": [
    "### Column Country\n",
    "\n",
    "I found this API that provides a list of country names, so I decided to use it to validate the values in the `Country` column of my dataset. I'll use this external source to ensure that the country names in my dataset are valid. For now, I'll be validating the country names only, focusing on whether the names match those provided by the API.\n",
    "\n",
    "#### Download the list\n",
    "Get a valid list from an url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92cf7951-6e6a-4be0-a230-6bf310b4ae58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Afghanistan', 'Albania', 'Algeria', 'AmericanSamoa', 'Andorra', 'Angola', 'Anguilla', 'Antarctica', 'Antigua and Barbuda', 'Argentina', 'Armenia', 'Aruba', 'Australia', 'Austria', 'Azerbaijan', 'Bahamas', 'Bahrain', 'Bangladesh', 'Barbados', 'Belarus', 'Belgium', 'Belize', 'Benin', 'Bermuda', 'Bhutan', 'Bolivia, Plurinational State of Bolivia', 'Bosnia and Herzegovina', 'Botswana', 'Bouvet Island', 'Brazil', 'British Indian Ocean Territory', 'Brunei Darussalam', 'Bulgaria', 'Burkina Faso', 'Burundi', 'Cambodia', 'Cameroon', 'Canada', 'Cape Verde', 'Cayman Islands', 'Central African Republic', 'Chad', 'Chile', 'China', 'Christmas Island', 'Cocos (Keeling) Islands', 'Colombia', 'Comoros', 'Congo', 'Congo, The Democratic Republic of the', 'Cook Islands', 'Costa Rica', 'Ivory Coast', 'Croatia', 'Cuba', 'Cyprus', 'Czech Republic', 'Denmark', 'Djibouti', 'Dominica', 'Dominican Republic', 'Ecuador', 'Egypt', 'El Salvador', 'Equatorial Guinea', 'Eritrea', 'Estonia', 'Ethiopia', 'Falkland Islands', 'Faroe Islands', 'Fiji', 'Finland', 'France', 'French Polynesia', 'French Southern and Antarctic Lands', 'Gabon', 'Gambia', 'Georgia', 'Germany', 'Ghana', 'Gibraltar', 'Greece', 'Greenland', 'Grenada', 'Guadeloupe', 'Guam', 'Guatemala', 'Guernsey', 'Guinea', 'Guinea-Bissau', 'Guyana', 'Haiti', 'Heard Island and McDonald Islands', 'Vatican City State (Holy See)', 'Honduras', 'Hong Kong', 'Hungary', 'Iceland', 'India', 'Indonesia', 'Iran, Islamic Republic of', 'Iraq', 'Ireland', 'Isle of Man', 'Israel', 'Italy', 'Jamaica', 'Japan', 'Jersey', 'Jordan', 'Kazakhstan', 'Kenya', 'Kiribati', 'North Korea', 'South Korea', 'Kuwait', 'Kyrgyzstan', 'Laos', 'Latvia', 'Lebanon', 'Lesotho', 'Liberia', 'Libyan Arab Jamahiriya', 'Liechtenstein', 'Lithuania', 'Luxembourg', 'Macau', 'Macedonia, The Former Yugoslav Republic of', 'Madagascar', 'Malawi', 'Malaysia', 'Maldives', 'Mali', 'Malta', 'Marshall Islands', 'Martinique', 'Mauritania', 'Mauritius', 'Mayotte', 'Mexico', 'Micronesia, Federated States of', 'Moldova, Republic of', 'Monaco', 'Mongolia', 'Montenegro', 'Montserrat', 'Morocco', 'Mozambique', 'Myanmar', 'Namibia', 'Nauru', 'Nepal', 'Netherlands', 'Netherlands Antilles', 'New Caledonia', 'New Zealand', 'Nicaragua', 'Niger', 'Nigeria', 'Niue', 'Norfolk Island', 'Northern Mariana Islands', 'Norway', 'Oman', 'Pakistan', 'Palau', 'Palestinian Territory, Occupied', 'Panama', 'Papua New Guinea', 'Paraguay', 'Peru', 'Philippines', 'Pitcairn', 'Poland', 'Portugal', 'Puerto Rico', 'Qatar', 'Réunion', 'Romania', 'Russia', 'Rwanda', 'Saint Helena, Ascension and Tristan Da Cunha', 'Saint Kitts and Nevis', 'Saint Lucia', 'Saint Pierre and Miquelon', 'Saint Vincent and the Grenadines', 'Samoa', 'San Marino', 'Sao Tome and Principe', 'Saudi Arabia', 'Senegal', 'Serbia', 'Seychelles', 'Sierra Leone', 'Singapore', 'Slovakia', 'Slovenia', 'Solomon Islands', 'Somalia', 'South Africa', 'South Georgia and the South Sandwich Islands', 'Spain', 'Sri Lanka', 'Sudan', 'Suriname', 'Svalbard and Jan Mayen', 'Swaziland', 'Sweden', 'Switzerland', 'Syria', 'Taiwan', 'Tajikistan', 'Tanzania, United Republic of', 'Thailand', 'Timor-Leste', 'Togo', 'Tokelau', 'Tonga', 'Trinidad and Tobago', 'Tunisia', 'Turkey', 'Turkmenistan', 'Turks and Caicos Islands', 'Tuvalu', 'Uganda', 'Ukraine', 'United Arab Emirates', 'United Kingdom', 'United States', 'United States Minor Outlying Islands', 'Uruguay', 'Uzbekistan', 'Vanuatu', 'Venezuela, Bolivarian Republic of', 'Vietnam', 'Virgin Islands, British', 'Virgin Islands, U.S.', 'Wallis and Futuna', 'Western Sahara', 'Yemen', 'Zambia', 'Zimbabwe']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Function to fetch and extract country names from the API\n",
    "def fetch_country_names(api_url):\n",
    "    # Make the GET request\n",
    "    try:\n",
    "        response = requests.get(api_url)\n",
    "        response.raise_for_status()  # Check for HTTP request errors\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching data: {e}\")\n",
    "        return []\n",
    "\n",
    "    # Extract the 'data' field (which contains the list of countries)\n",
    "    try:\n",
    "        data = response.json()\n",
    "        countries_data = data.get('data', [])\n",
    "    except ValueError as e:\n",
    "        print(f\"Error parsing JSON response: {e}\")\n",
    "        return []\n",
    "\n",
    "    # Extract the 'name' field from each country into a single list\n",
    "    return [country['name'] for country in countries_data if 'name' in country]\n",
    "\n",
    "# Define the API URL and fetch the country names\n",
    "url = \"https://countriesnow.space/api/v0.1/countries/positions\"\n",
    "country_names_list = fetch_country_names(url)\n",
    "\n",
    "# Display the list of country names\n",
    "print(country_names_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3c276d-0af2-455f-92e4-b318e988f94f",
   "metadata": {},
   "source": [
    "### Remove accents using `unidecode`\n",
    "I started by using the `unidecode` library to remove any accents or diacritics from the country names. This is important because accented characters can lead to inconsistencies when comparing or matching text, especially across different datasets.\n",
    "\n",
    "### Normalize country names by removing special characters\n",
    "Next, I removed any special characters (like punctuation). This ensures that variations like \"USA\" vs \"Usa\" or \"U.K.\" vs \"UK\" are treated consistently and won’t be considered different entities during the analysis.\n",
    "\n",
    "### Strip leading and trailing spaces\n",
    "Some country names might have had extra spaces either at the beginning or end. I applied `strip()` to remove those unnecessary spaces, ensuring cleaner data without unintended formatting issues.\n",
    "\n",
    "### Use regex to handle various cases of 'USA' and 'US'\n",
    "I used regular expressions to handle common variations of \"United States\" such as \"USA\", \"U.S.\", or \"America\". This step ensures that all these variations are unified into a single consistent label, \"United States\", which makes analysis much easier and avoids duplication.\n",
    "\n",
    "### Handle 'UK', 'Great Britain', 'Britain', and 'England' replacements in compound names\n",
    "Similarly, I handled multiple ways people might refer to the United Kingdom. Whether it’s \"UK\", \"Great Britain\", \"Britain\", or even \"England\" and \"Scotland\", I standardized these variations into a single label, \"United Kingdom\". This step ensures that all references to this region are treated consistently in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e87785b-5bcf-47be-b107-9446863ca56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import process\n",
    "from unidecode import unidecode\n",
    "\n",
    "# Ensure all country names in the dataframe are strings\n",
    "df_clean['Country'] = df_clean['Country'].astype(str)\n",
    "\n",
    "# Remove accents using unidecode and normalize country names to lowercase\n",
    "df_clean['Country'] = df_clean['Country'].apply(lambda x: unidecode(x)).str.replace(r'[^\\w\\s]', '', regex=True).str.strip()\n",
    "\n",
    "# Handle various cases of 'USA', 'US', 'Great Britain', 'UK', 'England', and 'Scotland'\n",
    "df_clean['Country'] = df_clean['Country'].str.replace(r'\\b(us|usa|usaa|the us|u\\.s\\.a|u\\.s\\.|america|usa )\\b', 'United States', regex=True, case=False)\n",
    "df_clean['Country'] = df_clean['Country'].str.replace(r'\\buk\\b|\\bgreat britain\\b|\\bbritain\\b|\\bengland\\b|\\benglanduk\\b|\\bscotland\\b', 'United Kingdom', case=False, regex=True)\n",
    "\n",
    "# Remove rows where the 'Country' has more than 6 words\n",
    "df_clean = df_clean[df_clean['Country'].apply(lambda x: len(x.split()) <= 6)]\n",
    "df_clean = df_clean[df_clean['Industry'].apply(lambda x: len(x.split()) <= 6)]\n",
    "\n",
    "# Lowercase the valid country names for comparison\n",
    "#country_names_list_lower = [unidecode(name).lower() for name in country_names_list]\n",
    "country_names_list_lower = country_names_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26dc0c6-1ee1-4024-a125-5229635baf70",
   "metadata": {},
   "source": [
    "### Check for invalid country names\n",
    "Comparing Country column with api list to find wrong/bad values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7de027a9-5159-4e90-98cb-3d42aa63e8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct invalid countries: ['', '217584year is deducted for benefits', 'Africa', 'Aotearoa New Zealand', 'Australi', 'Australian', 'Brasil', 'Burma', 'CANADA', 'California', 'Can', 'Canad', 'Canada Ottawa ontario', 'Canada and United States', 'Canadw', 'Canda', 'Catalonia', 'Contracts', 'Cote dIvoire', 'Csnada', 'Currently finance', 'Czech republic', 'Czechia', 'Danmark', 'Englang', 'FRANCE', 'Global', 'Hartford', 'INDIA', 'IS', 'ISA', 'International', 'Italy South', 'Japan United States Gov position', 'Jersey Channel islands', 'LOUTRELAND', 'London', 'Luxemburg', 'Mainland China', 'NIGERIA', 'NL', 'NZ', 'Nederland', 'New Zealand Aotearoa', 'New zealand', 'Northern Ireland', 'Northern Ireland United Kingdom', 'Policy', 'Remote', 'Remote philippines', 'SWITZERLAND', 'San Francisco', 'South africa', 'Sri lanka', 'The Bahamas', 'The Netherlands', 'The United States', 'The netherlands', 'U S', 'UA', 'UAE', 'UNITED STATES', 'UNited States', 'USAB', 'USD', 'USS', 'UXZ', 'Uniited States', 'Unite States', 'United  States', 'United Kindom', 'United Kingdom Gb', 'United Kingdom Northern Ireland', 'United Kingdom United Kingdom', 'United Kingdom for United States company', 'United Kingdom northern United Kingdom', 'United Kingdom remote', 'United Kingdomk', 'United STates', 'United Sates', 'United Sates of United States', 'United Stares', 'United State', 'United State of United States', 'United Statea', 'United Stated', 'United Stateds', 'United Statees', 'United States Of United States', 'United States Puerto Rico', 'United States Virgin Islands', 'United States but for foreign govt', 'United States is United States', 'United States of A', 'United States of American', 'United States of Americas', 'United States of United States', 'United States tomorrow', 'United Statesp', 'United Statss', 'United Stattes', 'United Statues', 'United Status', 'United Statws', 'United Sttes', 'United kingdom', 'United states', 'United states of United States', 'United statew', 'United y', 'UnitedStates', 'Uniteed States', 'Unitef Stated', 'Uniter Statez', 'Unites States', 'Unites kingdom', 'Unites states', 'Unitied States', 'Uniyed states', 'Uniyes States', 'Unted States', 'Untied States', 'Usat', 'Virginia', 'Wales', 'Wales United Kingdom', 'Y', 'australia', 'canada', 'croatia', 'czech republic', 'dbfemf', 'denmark', 'europe', 'ff', 'finland', 'france', 'germany', 'hong konh', 'ibdia', 'ireland', 'japan', 'na', 'na remote from wherever I want', 'netherlands', 'new zealand', 'pakistan', 'philippines', 'singapore', 'spain', 'ss', 'switzerland', 'the Netherlands', 'the netherlands', 'united States', 'united kingdom', 'united stated', 'united states', 'united states of United States']\n"
     ]
    }
   ],
   "source": [
    "# Function to validate and print distinct invalid countries\n",
    "def validate_countries():\n",
    "    # Find invalid countries\n",
    "    invalid_countries = df_clean[~df_clean['Country'].isin(country_names_list_lower)]\n",
    "    \n",
    "    # Print distinct and ordered invalid countries\n",
    "    distinct_invalid_countries = sorted([x for x in invalid_countries['Country'].unique() if isinstance(x, str)])\n",
    "    print(f\"Distinct invalid countries: {distinct_invalid_countries}\")\n",
    "    return invalid_countries\n",
    "\n",
    "# Validate countries before fuzzy matching\n",
    "invalid_countries = validate_countries()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03435781-3d06-411d-8042-7ea2a4d09654",
   "metadata": {},
   "source": [
    "**Comments:**\n",
    "\n",
    "With the result printed, I noticed there are some spelling mistakes or variations in country names, like 'csnada' instead of 'Canada', 'danmark' instead of 'Denmark', and 'united kingdomk' instead of 'United Kingdom'. To address these issues, I decided to use **fuzzy matching**. Fuzzy matching is a method that helps find the closest match between two strings, which is particularly useful when dealing with typos, spelling errors, or different formats. By applying fuzzy matching, I aim to correct these mistakes and standardize the country names in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab911362-0b9c-4cc2-abab-e9e89e649fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched 'Africa' to 'Central African Republic' with score 90\n",
      "Matched 'Aotearoa New Zealand' to 'New Zealand' with score 90\n",
      "Matched 'Australi' to 'Australia' with score 94\n",
      "Matched 'Australian' to 'Australia' with score 95\n",
      "Matched 'Brasil' to 'Brazil' with score 83\n",
      "Matched 'CANADA' to 'Canada' with score 100\n",
      "Matched 'Canad' to 'Canada' with score 91\n",
      "Matched 'Canada Ottawa ontario' to 'Canada' with score 90\n",
      "Matched 'Canada and United States' to 'Canada' with score 90\n",
      "Matched 'Canadw' to 'Canada' with score 83\n",
      "Matched 'Canda' to 'Canada' with score 91\n",
      "Matched 'Csnada' to 'Canada' with score 83\n",
      "Matched 'Czech republic' to 'Czech Republic' with score 100\n",
      "Matched 'Danmark' to 'Denmark' with score 86\n",
      "Matched 'FRANCE' to 'France' with score 100\n",
      "Matched 'INDIA' to 'India' with score 100\n",
      "Matched 'Italy South' to 'Italy' with score 90\n",
      "Matched 'Japan United States Gov position' to 'Japan' with score 90\n",
      "Matched 'Jersey Channel islands' to 'Jersey' with score 90\n",
      "Matched 'Luxemburg' to 'Luxembourg' with score 95\n",
      "Matched 'Mainland China' to 'China' with score 90\n",
      "Matched 'NIGERIA' to 'Nigeria' with score 100\n",
      "Matched 'Nederland' to 'Netherlands' with score 80\n",
      "Matched 'New Zealand Aotearoa' to 'New Zealand' with score 90\n",
      "Matched 'New zealand' to 'New Zealand' with score 100\n",
      "Matched 'Northern Ireland' to 'Ireland' with score 90\n",
      "Matched 'Northern Ireland United Kingdom' to 'Ireland' with score 90\n",
      "Matched 'Remote philippines' to 'Philippines' with score 90\n",
      "Matched 'SWITZERLAND' to 'Switzerland' with score 100\n",
      "Matched 'South africa' to 'South Africa' with score 100\n",
      "Matched 'Sri lanka' to 'Sri Lanka' with score 100\n",
      "Matched 'The Bahamas' to 'Bahamas' with score 90\n",
      "Matched 'The Netherlands' to 'Netherlands' with score 95\n",
      "Matched 'The United States' to 'United States' with score 95\n",
      "Matched 'The netherlands' to 'Netherlands' with score 95\n",
      "Matched 'UNITED STATES' to 'United States' with score 100\n",
      "Matched 'UNited States' to 'United States' with score 100\n",
      "Matched 'Uniited States' to 'United States' with score 96\n",
      "Matched 'Unite States' to 'United States' with score 96\n",
      "Matched 'United  States' to 'United States' with score 96\n",
      "Matched 'United Kindom' to 'United Kingdom' with score 96\n",
      "Matched 'United Kingdom Gb' to 'United Kingdom' with score 95\n",
      "Matched 'United Kingdom Northern Ireland' to 'Ireland' with score 90\n",
      "Matched 'United Kingdom United Kingdom' to 'United Kingdom' with score 90\n",
      "Matched 'United Kingdom for United States company' to 'United Kingdom' with score 90\n",
      "Matched 'United Kingdom northern United Kingdom' to 'United Kingdom' with score 90\n",
      "Matched 'United Kingdom remote' to 'United Kingdom' with score 90\n",
      "Matched 'United Kingdomk' to 'United Kingdom' with score 97\n",
      "Matched 'United STates' to 'United States' with score 100\n",
      "Matched 'United Sates' to 'United States' with score 96\n",
      "Matched 'United Sates of United States' to 'United States' with score 90\n",
      "Matched 'United Stares' to 'United States' with score 92\n",
      "Matched 'United State' to 'United States' with score 96\n",
      "Matched 'United State of United States' to 'United States' with score 90\n",
      "Matched 'United Statea' to 'United States' with score 92\n",
      "Matched 'United Stated' to 'United States' with score 92\n",
      "Matched 'United Stateds' to 'United States' with score 96\n",
      "Matched 'United Statees' to 'United States' with score 96\n",
      "Matched 'United States Of United States' to 'United States' with score 90\n",
      "Matched 'United States Puerto Rico' to 'Puerto Rico' with score 90\n",
      "Matched 'United States Virgin Islands' to 'United States' with score 90\n",
      "Matched 'United States but for foreign govt' to 'United States' with score 90\n",
      "Matched 'United States is United States' to 'United States' with score 90\n",
      "Matched 'United States of A' to 'United States' with score 95\n",
      "Matched 'United States of American' to 'United States' with score 90\n",
      "Matched 'United States of Americas' to 'United States' with score 90\n",
      "Matched 'United States of United States' to 'United States' with score 90\n",
      "Matched 'United States tomorrow' to 'United States' with score 90\n",
      "Matched 'United Statesp' to 'United States' with score 96\n",
      "Matched 'United Statss' to 'United States' with score 92\n",
      "Matched 'United Stattes' to 'United States' with score 96\n",
      "Matched 'United Statues' to 'United States' with score 96\n",
      "Matched 'United Status' to 'United States' with score 92\n",
      "Matched 'United Statws' to 'United States' with score 92\n",
      "Matched 'United Sttes' to 'United States' with score 96\n",
      "Matched 'United kingdom' to 'United Kingdom' with score 100\n",
      "Matched 'United states' to 'United States' with score 100\n",
      "Matched 'United states of United States' to 'United States' with score 90\n",
      "Matched 'United statew' to 'United States' with score 92\n",
      "Matched 'United y' to 'Tanzania, United Republic of' with score 86\n",
      "Matched 'UnitedStates' to 'United States' with score 96\n",
      "Matched 'Uniteed States' to 'United States' with score 96\n",
      "Matched 'Unitef Stated' to 'United States' with score 85\n",
      "Matched 'Uniter Statez' to 'United States' with score 85\n",
      "Matched 'Unites States' to 'United States' with score 92\n",
      "Matched 'Unites kingdom' to 'United Kingdom' with score 93\n",
      "Matched 'Unites states' to 'United States' with score 92\n",
      "Matched 'Unitied States' to 'United States' with score 96\n",
      "Matched 'Uniyed states' to 'United States' with score 92\n",
      "Matched 'Uniyes States' to 'Micronesia, Federated States of' with score 86\n",
      "Matched 'Unted States' to 'United States' with score 96\n",
      "Matched 'Untied States' to 'United States' with score 92\n",
      "Matched 'Virginia' to 'India' with score 80\n",
      "Matched 'Wales United Kingdom' to 'United Kingdom' with score 95\n",
      "Matched 'australia' to 'Australia' with score 100\n",
      "Matched 'canada' to 'Canada' with score 100\n",
      "Matched 'croatia' to 'Croatia' with score 100\n",
      "Matched 'czech republic' to 'Czech Republic' with score 100\n",
      "Matched 'denmark' to 'Denmark' with score 100\n",
      "Matched 'finland' to 'Finland' with score 100\n",
      "Matched 'france' to 'France' with score 100\n",
      "Matched 'germany' to 'Germany' with score 100\n",
      "Matched 'hong konh' to 'Hong Kong' with score 89\n",
      "Matched 'ibdia' to 'India' with score 80\n",
      "Matched 'ireland' to 'Ireland' with score 100\n",
      "Matched 'japan' to 'Japan' with score 100\n",
      "Matched 'netherlands' to 'Netherlands' with score 100\n",
      "Matched 'new zealand' to 'New Zealand' with score 100\n",
      "Matched 'pakistan' to 'Pakistan' with score 100\n",
      "Matched 'philippines' to 'Philippines' with score 100\n",
      "Matched 'singapore' to 'Singapore' with score 100\n",
      "Matched 'spain' to 'Spain' with score 100\n",
      "Matched 'switzerland' to 'Switzerland' with score 100\n",
      "Matched 'the Netherlands' to 'Netherlands' with score 95\n",
      "Matched 'the netherlands' to 'Netherlands' with score 95\n",
      "Matched 'united States' to 'United States' with score 100\n",
      "Matched 'united kingdom' to 'United Kingdom' with score 100\n",
      "Matched 'united stated' to 'United States' with score 92\n",
      "Matched 'united states' to 'United States' with score 100\n",
      "Matched 'united states of United States' to 'United States' with score 90\n"
     ]
    }
   ],
   "source": [
    "# Apply fuzzy matching only to the rows with invalid country names\n",
    "def fuzzy_match_country(country):    \n",
    "    if len(country) >= 4:        \n",
    "        match, score = process.extractOne(country, country_names_list_lower)                \n",
    "        if score >= 80:  # Set a threshold for fuzzy matching accuracy\n",
    "            print(f\"Matched '{country}' to '{match}' with score {score}\")\n",
    "            return match\n",
    "        else:\n",
    "            return 'Unknown'\n",
    "    return 'Unknown'\n",
    "\n",
    "# Apply fuzzy matching to distinct invalid country names\n",
    "distinct_invalid_countries = sorted([x for x in invalid_countries['Country'].unique() if isinstance(x, str)])\n",
    "\n",
    "# Create fuzzy matched dictionary\n",
    "fuzzy_matched_dict = {country: fuzzy_match_country(country) for country in distinct_invalid_countries}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f4b776-4dc2-4d5a-b5a3-5236b6644bae",
   "metadata": {},
   "source": [
    "### Remove invalid countries from dataset\n",
    "\n",
    "In this step, I updated the invalid country names in the dataframe by applying the fuzzy matching results. This was done using the `fuzzy_matched_dict` to map the incorrect country names to their correct counterparts. The reason for this is to ensure that the dataset contains clean and accurate country information.\n",
    "\n",
    "I found the number of 'Unknown' countries and printed it before removing them of my dataset.\n",
    "\n",
    "Finally, I re-ran the validation function to ensure there are no remaining invalid country names after this step.\n",
    "\n",
    "63 rows removed from dataset as result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f582bfb8-7d5f-4a7d-8206-2819e8507119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with 'Unknown' Contry name: 63. They will be deleted from dataset\n",
      "Distinct invalid countries: []\n"
     ]
    }
   ],
   "source": [
    "# Update invalid countries in the dataframe using the fuzzy matched dictionary\n",
    "df_clean.loc[invalid_countries.index, 'Country'] = df_clean['Country'].map(fuzzy_matched_dict)\n",
    "\n",
    "# Count rows where 'Country' is 'Unknown'\n",
    "unknown_country_count = df_clean[df_clean['Country'] == 'Unknown'].shape[0]\n",
    "\n",
    "print(f\"Number of rows with 'Unknown' Contry name: {unknown_country_count}. They will be deleted from dataset\")\n",
    "\n",
    "df_clean = df_clean[df_clean['Country'] != 'Unknown']\n",
    "\n",
    "# Re-validate countries after applying fuzzy matching\n",
    "invalid_countries = validate_countries()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24dc0b8-4c20-43d6-8452-7f53be2c83f4",
   "metadata": {},
   "source": [
    "### Column Industry\n",
    "\n",
    "In the Industry column, we have over 1200 unique values. To simplify the dataset and make it more manageable for analysis, we decided to create a new \"Industry Category\" column. This new column groups similar industry values together, significantly reducing the number of unique categories while preserving the integrity of the data. This helps in making the data more structured and easier to work with for future analysis or reporting.\n",
    "\n",
    "Firstly, let's save an unique industry list into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88cb71e1-f32c-434a-ad2d-a445884a35a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ind_values = sorted(df_clean['Industry'].unique())\n",
    "df_industries = pd.DataFrame({'Industry': unique_ind_values})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07439535-5643-4775-a19e-bfcbcc8e658c",
   "metadata": {},
   "source": [
    "### Industry Clustering Process\n",
    "\n",
    "In this analysis, we processed the `Industry` column to reduce the number of unique categories. Here's the approach:\n",
    "\n",
    "1. **Text Preprocessing**: I started by normalizing the text. This involved converting everything to lowercase, removing special characters and redundant spaces, and applying some specific rules like replacing \"science/research\" patterns with \"science research.\" Additionally, I used lemmatization to reduce words to their root form, improving consistency across similar terms.\n",
    "   \n",
    "2. **Embedding Generation**: I utilized the paraphrase-MiniLM-L6-v2 model to encode the processed industry names into embeddings. These embeddings help capture the semantic meaning behind the industry names, allowing for more effective clustering of similar terms.\n",
    "\n",
    "3. **Clustering**: I experimented with different distance metrics (`cosine`, `euclidean`, `manhattan`) to see how each method grouped the industry names based on their similarity.\n",
    "\n",
    "4. **Results**: Let's check below the resulf of each metric:\n",
    "\n",
    "- Metric: euclidean - Unique Representative Categories: 855\n",
    "- Metric: manhattan - Unique Representative Categories: 855\n",
    "- Metric: cosine - Unique Representative Categories: 388\n",
    "\n",
    "Let's use 'cosine' metric from now on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31ff8a01-e69b-4dfc-b162-f9cfdc891aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric: euclidean - Unique Representative Categories: 855\n",
      "Metric: manhattan - Unique Representative Categories: 855\n",
      "Metric: cosine - Unique Representative Categories: 388\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as sklearn_stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "def preprocess_text(text, stop_words, lemmatizer):\n",
    "    \"\"\"\n",
    "    Preprocesses the input text by normalizing case, removing special characters, \n",
    "    applying custom rules, and performing lemmatization.\n",
    "\n",
    "    Args:\n",
    "        text (str): The raw input text (industry name).\n",
    "        stop_words (set): A set of stopwords to remove.\n",
    "        lemmatizer (WordNetLemmatizer): NLTK lemmatizer to reduce words to their base forms.\n",
    "\n",
    "    Returns:\n",
    "        str: Preprocessed text.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = text.replace('/', ' ')\n",
    "    text = re.sub(r'\\(.*?\\)', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'scien\\w*\\sresearch\\w*.*', 'science research', text)\n",
    "    text = re.sub(r'\\breal\\s*estate\\w*.*', 'Real Estate', text)\n",
    "    \n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def encode_industry_names(industry_names, model):\n",
    "    \"\"\"\n",
    "    Encodes the industry names into embeddings using a pre-trained sentence transformer model.\n",
    "\n",
    "    Args:\n",
    "        industry_names (pd.Series): A pandas Series of industry names.\n",
    "        model (SentenceTransformer): Pre-trained SentenceTransformer model.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The embeddings for the industry names.\n",
    "    \"\"\"\n",
    "    return model.encode(industry_names.values)\n",
    "\n",
    "def compute_cosine_similarity(embeddings, single_embedding=None):\n",
    "    if single_embedding is not None:\n",
    "        # If comparing a single embedding against a set of embeddings\n",
    "        return cosine_similarity(single_embedding, embeddings)\n",
    "    else:\n",
    "        # Compute pairwise cosine similarity for all embeddings\n",
    "        return 1 - cosine_similarity(embeddings)\n",
    "\n",
    "def perform_clustering(embeddings, df, metric, distance_threshold=0.4):\n",
    "    \"\"\"\n",
    "    Performs Agglomerative Clustering on the industry embeddings and assigns cluster labels.\n",
    "\n",
    "    Args:\n",
    "        embeddings (np.ndarray): Encoded industry embeddings.\n",
    "        df (pd.DataFrame): The original DataFrame of industries.\n",
    "        metric (str): The metric to use for clustering (e.g., 'cosine', 'euclidean', 'manhattan').\n",
    "        distance_threshold (float): The distance threshold for clustering.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with cluster labels and representative categories.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    if metric == 'cosine':\n",
    "        cosine_distances = compute_cosine_similarity(embeddings)\n",
    "        clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=distance_threshold, metric='precomputed', linkage='complete')\n",
    "        clusters = clustering.fit_predict(cosine_distances)\n",
    "    else:\n",
    "        clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=distance_threshold, metric=metric, linkage='complete')\n",
    "        clusters = clustering.fit_predict(embeddings)\n",
    "    \n",
    "    # Assign clusters to the copy of the DataFrame\n",
    "    df_copy['Cluster'] = clusters\n",
    "    \n",
    "    # Assign representative categories based on the first item in each cluster\n",
    "    df_copy['Industry Category'] = df_copy.groupby('Cluster')['Industry'].transform('first')\n",
    "    \n",
    "    # Calculate the number of unique categories\n",
    "    unique_categories_count = df_copy['Industry Category'].nunique()\n",
    "    \n",
    "    # Display the metric and the number of unique categories\n",
    "    print(f\"Metric: {metric} - Unique Representative Categories: {unique_categories_count}\")\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "def run_clustering(df_industries, model, metrics):\n",
    "    \"\"\"\n",
    "    Runs the clustering process for each provided metric and displays the results.\n",
    "\n",
    "    Args:\n",
    "        df_industries (pd.DataFrame): The original DataFrame with industry names.\n",
    "        model (SentenceTransformer): Pre-trained SentenceTransformer model.\n",
    "        metrics (list): List of metrics to use for clustering.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Preprocess industry names\n",
    "    stop_words = sklearn_stopwords\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    df_industries['Processed_Industry'] = df_industries['Industry'].apply(preprocess_text, args=(stop_words, lemmatizer))\n",
    "\n",
    "    # Encode industry names\n",
    "    industry_embeddings = encode_industry_names(df_industries['Processed_Industry'], model)\n",
    "\n",
    "    # Run clustering for each metric\n",
    "    clustered_df = None\n",
    "    for metric in metrics:\n",
    "        clustered_df = perform_clustering(industry_embeddings, df_industries, metric)        \n",
    "    \n",
    "    # Return the final clustered DataFrame\n",
    "    return clustered_df\n",
    "\n",
    "# Load pre-trained model and define metrics\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "metrics = ['euclidean', 'manhattan', 'cosine']\n",
    "\n",
    "# Run clustering\n",
    "x = run_clustering(df_industries, model, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f71c0c6-fcd6-4278-a0ea-87151b8491d0",
   "metadata": {},
   "source": [
    "**Comments:**\n",
    "\n",
    "In the previous cell, I applied some specific rules for handling patterns like 'science research' and 'real estate.' The cosine similarity method wasn't sufficient to correctly group these terms, so I manually introduced mappings to ensure better categorization:\n",
    "\n",
    "- `r'scien\\w*\\sresearch\\w*.*'` was mapped to 'science research'\n",
    "- `r'\\breal\\s*estate\\w*.*'` was mapped to 'Real Estate'\n",
    "\n",
    "These adjustments allowed for more accurate grouping of similar industry names that were not effectively handled by the cosine similarity alone.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a041006b-871c-4bc8-a1f2-23ef0500de2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric: cosine - Unique Representative Categories: 388\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Age Group</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Annual Salary</th>\n",
       "      <th>Additional Compensation</th>\n",
       "      <th>Currency</th>\n",
       "      <th>Country</th>\n",
       "      <th>State</th>\n",
       "      <th>City</th>\n",
       "      <th>Total Experience</th>\n",
       "      <th>Field Experience</th>\n",
       "      <th>Education Level</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Race</th>\n",
       "      <th>Industry Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-04-27 11:02:09.743</td>\n",
       "      <td>25-34</td>\n",
       "      <td>Education (Higher Education)</td>\n",
       "      <td>Research and Instruction Librarian</td>\n",
       "      <td>55000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>USD</td>\n",
       "      <td>United States</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>Boston</td>\n",
       "      <td>5-7 years</td>\n",
       "      <td>5-7 years</td>\n",
       "      <td>Master's degree</td>\n",
       "      <td>Woman</td>\n",
       "      <td>White</td>\n",
       "      <td>Adult education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-04-27 11:02:21.562</td>\n",
       "      <td>25-34</td>\n",
       "      <td>Computing or Tech</td>\n",
       "      <td>Change &amp; Internal Communications Manager</td>\n",
       "      <td>54600</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>GBP</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Cambridge</td>\n",
       "      <td>8 - 10 years</td>\n",
       "      <td>5-7 years</td>\n",
       "      <td>College degree</td>\n",
       "      <td>Non-binary</td>\n",
       "      <td>White</td>\n",
       "      <td>Computing or Tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-04-27 11:02:38.125</td>\n",
       "      <td>25-34</td>\n",
       "      <td>Accounting, Banking &amp; Finance</td>\n",
       "      <td>Marketing Specialist</td>\n",
       "      <td>34000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>USD</td>\n",
       "      <td>United States</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>Chattanooga</td>\n",
       "      <td>2 - 4 years</td>\n",
       "      <td>2 - 4 years</td>\n",
       "      <td>College degree</td>\n",
       "      <td>Woman</td>\n",
       "      <td>White</td>\n",
       "      <td>Accounting, Banking &amp; Finance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-04-27 11:02:40.643</td>\n",
       "      <td>25-34</td>\n",
       "      <td>Nonprofits</td>\n",
       "      <td>Program Manager</td>\n",
       "      <td>62000</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>USD</td>\n",
       "      <td>United States</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>Milwaukee</td>\n",
       "      <td>8 - 10 years</td>\n",
       "      <td>5-7 years</td>\n",
       "      <td>College degree</td>\n",
       "      <td>Woman</td>\n",
       "      <td>White</td>\n",
       "      <td>Nonprofit - legal department</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-04-27 11:02:41.793</td>\n",
       "      <td>25-34</td>\n",
       "      <td>Accounting, Banking &amp; Finance</td>\n",
       "      <td>Accounting Manager</td>\n",
       "      <td>60000</td>\n",
       "      <td>7000.0</td>\n",
       "      <td>USD</td>\n",
       "      <td>United States</td>\n",
       "      <td>South Carolina</td>\n",
       "      <td>Greenville</td>\n",
       "      <td>8 - 10 years</td>\n",
       "      <td>5-7 years</td>\n",
       "      <td>College degree</td>\n",
       "      <td>Woman</td>\n",
       "      <td>White</td>\n",
       "      <td>Accounting, Banking &amp; Finance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021-04-27 11:02:45.571</td>\n",
       "      <td>25-34</td>\n",
       "      <td>Education (Higher Education)</td>\n",
       "      <td>Scholarly Publishing Librarian</td>\n",
       "      <td>62000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>USD</td>\n",
       "      <td>United States</td>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>Hanover</td>\n",
       "      <td>8 - 10 years</td>\n",
       "      <td>2 - 4 years</td>\n",
       "      <td>Master's degree</td>\n",
       "      <td>Man</td>\n",
       "      <td>White</td>\n",
       "      <td>Adult education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021-04-27 11:02:50.507</td>\n",
       "      <td>25-34</td>\n",
       "      <td>Publishing</td>\n",
       "      <td>Publishing Assistant</td>\n",
       "      <td>33000</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>USD</td>\n",
       "      <td>United States</td>\n",
       "      <td>South Carolina</td>\n",
       "      <td>Columbia</td>\n",
       "      <td>2 - 4 years</td>\n",
       "      <td>2 - 4 years</td>\n",
       "      <td>College degree</td>\n",
       "      <td>Woman</td>\n",
       "      <td>White</td>\n",
       "      <td>Academic Publishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021-04-27 11:02:59.927</td>\n",
       "      <td>25-34</td>\n",
       "      <td>Education (Primary/Secondary)</td>\n",
       "      <td>Librarian</td>\n",
       "      <td>50000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>USD</td>\n",
       "      <td>United States</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>Yuma</td>\n",
       "      <td>5-7 years</td>\n",
       "      <td>5-7 years</td>\n",
       "      <td>Master's degree</td>\n",
       "      <td>Man</td>\n",
       "      <td>White</td>\n",
       "      <td>Adult education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2021-04-27 11:03:01.045</td>\n",
       "      <td>45-54</td>\n",
       "      <td>Computing or Tech</td>\n",
       "      <td>Systems Analyst</td>\n",
       "      <td>112000</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>USD</td>\n",
       "      <td>United States</td>\n",
       "      <td>Missouri</td>\n",
       "      <td>St. Louis</td>\n",
       "      <td>21 - 30 years</td>\n",
       "      <td>21 - 30 years</td>\n",
       "      <td>College degree</td>\n",
       "      <td>Woman</td>\n",
       "      <td>White</td>\n",
       "      <td>Computing or Tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2021-04-27 11:03:01.699</td>\n",
       "      <td>35-44</td>\n",
       "      <td>Accounting, Banking &amp; Finance</td>\n",
       "      <td>Senior Accountant</td>\n",
       "      <td>45000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>USD</td>\n",
       "      <td>United States</td>\n",
       "      <td>Florida</td>\n",
       "      <td>Palm Coast</td>\n",
       "      <td>21 - 30 years</td>\n",
       "      <td>21 - 30 years</td>\n",
       "      <td>College degree</td>\n",
       "      <td>Woman</td>\n",
       "      <td>Hispanic, Latino, or Spanish origin, White</td>\n",
       "      <td>Accounting, Banking &amp; Finance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Timestamp               Age Group Industry                       Job Title                                  Annual Salary  Additional Compensation Currency Country         State           City         Total Experience Field Experience Education Level  Gender      Race                                        Industry Category             \n",
       "0 2021-04-27 11:02:09.743  25-34      Education (Higher Education)        Research and Instruction Librarian   55000             0.0                  USD       United States   Massachusetts       Boston      5-7 years        5-7 years    Master's degree       Woman                                       White                Adult education\n",
       "1 2021-04-27 11:02:21.562  25-34                 Computing or Tech  Change & Internal Communications Manager   54600          4000.0                  GBP      United Kingdom         Unknown    Cambridge   8 - 10 years        5-7 years     College degree  Non-binary                                       White              Computing or Tech\n",
       "2 2021-04-27 11:02:38.125  25-34     Accounting, Banking & Finance                      Marketing Specialist   34000             0.0                  USD       United States       Tennessee  Chattanooga    2 - 4 years      2 - 4 years     College degree       Woman                                       White  Accounting, Banking & Finance\n",
       "3 2021-04-27 11:02:40.643  25-34                        Nonprofits                           Program Manager   62000          3000.0                  USD       United States       Wisconsin    Milwaukee   8 - 10 years        5-7 years     College degree       Woman                                       White   Nonprofit - legal department\n",
       "4 2021-04-27 11:02:41.793  25-34     Accounting, Banking & Finance                        Accounting Manager   60000          7000.0                  USD       United States  South Carolina   Greenville   8 - 10 years        5-7 years     College degree       Woman                                       White  Accounting, Banking & Finance\n",
       "5 2021-04-27 11:02:45.571  25-34      Education (Higher Education)            Scholarly Publishing Librarian   62000             0.0                  USD       United States   New Hampshire      Hanover   8 - 10 years      2 - 4 years    Master's degree         Man                                       White                Adult education\n",
       "6 2021-04-27 11:02:50.507  25-34                        Publishing                      Publishing Assistant   33000          2000.0                  USD       United States  South Carolina     Columbia    2 - 4 years      2 - 4 years     College degree       Woman                                       White            Academic Publishing\n",
       "7 2021-04-27 11:02:59.927  25-34     Education (Primary/Secondary)                                 Librarian   50000             0.0                  USD       United States         Arizona         Yuma      5-7 years        5-7 years    Master's degree         Man                                       White                Adult education\n",
       "8 2021-04-27 11:03:01.045  45-54                 Computing or Tech                           Systems Analyst  112000         10000.0                  USD       United States        Missouri    St. Louis  21 - 30 years    21 - 30 years     College degree       Woman                                       White              Computing or Tech\n",
       "9 2021-04-27 11:03:01.699  35-44     Accounting, Banking & Finance                         Senior Accountant   45000             0.0                  USD       United States         Florida   Palm Coast  21 - 30 years    21 - 30 years     College degree       Woman  Hispanic, Latino, or Spanish origin, White  Accounting, Banking & Finance"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = ['cosine']\n",
    "\n",
    "df_best_metric = run_clustering(df_industries, model, metrics)\n",
    "\n",
    "df_clean = pd.merge(df_clean, df_best_metric[['Industry', 'Industry Category']], on='Industry', how='left')\n",
    "\n",
    "df_clean.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed43a74d-5817-418a-b602-5e15f8c30bc0",
   "metadata": {},
   "source": [
    "### QA\n",
    "\n",
    "Check below new 'Industry Category' has 388 unique values but it is not good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a02d36a1-1461-4976-92cd-c3ba6ff1f1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp                  27966\n",
      "Age Group                      7\n",
      "Industry                    1185\n",
      "Job Title                  14302\n",
      "Annual Salary               3654\n",
      "Additional Compensation      844\n",
      "Currency                      11\n",
      "Country                       97\n",
      "State                        134\n",
      "City                        4798\n",
      "Total Experience               8\n",
      "Field Experience               8\n",
      "Education Level                6\n",
      "Gender                         5\n",
      "Race                          50\n",
      "Industry Category            388\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Display the number of distinct values in each column\n",
    "distinct_values_count = df_clean.nunique()\n",
    "\n",
    "print(distinct_values_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835ed2cc-8d6c-48fb-bdf1-20f3f96a4261",
   "metadata": {},
   "source": [
    "### Industry Category Enhancement\n",
    "\n",
    "There are some categories with just 1 or 2 rows. I want to realocate them to another existing category. Let's do it with AI to avoid manual mapping.\n",
    "\n",
    "Industry categories have 162 unique values as result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f059b4b6-cd2b-44b6-aa1c-299a9a870c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Online education': 'Adult education', 'Professional Association ': 'Association', 'Purchasing': ' Buyer', 'Analytics': 'Information ', 'Cleaning': 'Maintenance', 'Conservation': 'Environmental', 'Plumbing': 'Maintenance', 'Psychologist': 'Scientist', 'Customer service/publishing-adjacent ': 'Customer Service', 'Security and manufacturing company': 'IT Security', 'Wine Importing/Distribution': 'Beverage Production', 'Library--public': 'Archives/Libraries', 'Government Research': 'Government ', 'Fast Food': 'Food', 'Internet': 'Telecommunications', 'CPG': 'Medical Communications', 'Counseling': 'Mental Health', 'Horticulture': 'Agriculture or Forestry', 'Auction House': ' Buyer', 'Education start-up': 'Early Childhood Education', 'Concrete': 'Construction', 'Costruction admin': 'Administration', 'Virtual Assisting ': 'Digital', 'Enviromental ': 'Environment', 'CALL CENTER': 'Customer Service', 'Public Health, local government': 'Health care', 'Think tank': 'Energy - Oil and Gas', 'Tabletop Games Publishing': 'Game Development', 'Landscape Architecture': 'Architect', 'Animal Caretaker': 'Animal Care', 'User Experience (UX) Research': 'Consumer Research', 'R&D': 'Pharma R&D', 'Professional regulation': 'Federal Contracting/Business Development', 'Environmental Restoration': 'Environmental', 'Private Equity': 'Commercial Real Estate', 'Wine': 'Beverage', 'Healthcare Information Technology': 'Medical Communications', 'Professional services / architecture ': 'Business Services', 'Contract Research': 'Federal Contracting/Business Development', 'FinTech/Payment Processing': 'Fintech', 'National laboratory': 'Analytical lab', 'not-for-profit health research consulting': 'Health Research', 'Technical writing': 'Academic Publishing', 'Interior Design & Architecture': 'Architect', 'Executive Search': 'Administration', 'Large University Administration': 'Academia', 'Learning & Development': 'Instructional Design and Training', 'Commercial furniture': 'Non Profit Theater', 'Communications/publications': 'Communication Research', 'Organizational Development ': 'Federal Contracting/Business Development', 'Medica education': 'Academic Medicine ', 'Animal Health Product Manufacturing': 'Animal Care', 'Community Foundation': 'Charity', 'FMCG': 'Medical Communications', 'Environmental Health + Pest Control': 'Environment, health, and safety', 'For profit education': 'Education service provider', 'Wholesale - Apparel': 'Apparel', 'Affordable Housing Real Estate Development (nonprofit)': 'Commercial Real Estate', 'Quality Assurance': 'Compliance', 'Corporate Learning and Development': 'Federal Contracting/Business Development', 'Intergovernmental organization': 'International Development', 'Corporate Sustainability ': 'Consumer Product Organization ', 'Aerospace and Defense/Government Contracting': 'Defense Contracting', 'Beauty Manufacturing & Education ': 'Beauty', 'B2B Services ': 'Business Services', 'Brewing': 'Beverage Production', 'Government- Scientist': 'Scientist', 'Accessibility': 'Education- museum/public outreach ', 'Paid student intern in Tech': 'Ed Tech', 'Language Services': 'Education service provider', 'Emergency Management': 'Program management ', 'Surveying': 'Commercial Landscaping ', 'Intelligence': 'Information ', 'Content Review - Copyright/DMCA': 'Academic Publishing', 'Strategy': 'Policy research', 'Probiotics': 'Biopharma', 'Public Opinion Research': 'Policy research', 'Per Sitter': 'HRO', 'Work-Study': 'Research', 'e-comm': 'E-Commerce', 'Landed Estate': 'Commercial Real Estate', 'Performing Arts': 'Arts, Culture and Heritage', 'Eap': 'Ed Tech', 'Fire protection': 'IT Security', 'parking ': 'Automotive', 'Toxicology': 'Medical/Pharmaceutical ', 'Preclinical Research': 'Research', 'Music Licensing': 'Music', 'Sports': 'Leisure, Sport & Tourism', 'Textbook Copyeditor': 'Academic Publishing', 'Janitorial': 'Maintenance', 'Funding Intermediary ': 'Nonprofit - legal department', 'Auto Mfg.': 'Auto Repair', 'Soft Drinks Manufacturing ': 'Beverage Production', 'economics': 'Logistics', 'Beer sales': 'Beverage Production', 'Diversity, Equity & Inclusion ': 'Arts, Culture and Heritage', 'Data Breach': 'Diagnostic Medical Devices', 'Music: freelance, performing and education': 'Music', 'Survey methodology': 'Research', 'Repair facility for heavy duty trucks': 'Auto Repair', 'Ipr': 'Telecommunications', 'Bitech': 'Food', 'Wealth advisor Research': 'Academic/nonprofit research', 'coaching': 'Career & Technical Training', 'MedComms': 'Medical Communications', 'PhD': 'Academic Medicine ', 'Investing': 'Fintech', 'Rural electrification': 'Energy / renewables', 'Sailing Instructor': 'Instructional Design and Training', 'Env. Consulting': 'Business or Consulting', 'Literature': 'Research', 'Retired': 'Archives', 'Animation': 'Art & Design', 'Obligatory Military service': 'Military', 'Maritime': 'Military', 'Forklift operator warehouse': 'Logistics', 'Administration in MLM': 'Administration', 'Clinical Research Manager - academic institution': 'Clinical Research', 'Rideshare': 'Automotive', 'Homemaker': ' Buyer', 'Bioinformatics ': 'Biological Sciences', 'Foodservice': 'Food Service', 'Space': 'IT', 'ESL Teacher': 'Adult education', 'Commercial Fisherman': 'Logistics', 'Oilfield adjacent': 'Energy - Oil and Gas', 'Operations': 'Construction', 'Beauty, Cosmetics, Fragrance': 'Beauty', 'Contact Center': 'Customer Service', 'College Athletics': 'Academic Science ', 'E commerce ': 'E-Commerce', 'Graduate assistant and also events ': 'Graduate Student', 'Laundry and Rental': 'Haz/Ind/Rad Waste Management ', 'Consulting Operations- Big 4': 'Business or Consulting', 'Grantwriting Consultants': 'Business or Consulting', 'Pre-primary education': 'Early Childhood Education', 'Skilled trade ': 'Construction, mining, manufacturing', 'Delivery and installation for commercial machinery': 'Procurement', 'Publishibg': 'Academic Publishing', 'Background Screening': 'STEM Research', 'Earth sciences': 'Life Sciences', 'Freelance/Self-Employed Consultant ': 'Freelance Journalism', 'Film Post-Production': 'Non Profit Theater', 'Coffee - Importing': 'Beverage Production', 'Automotive finance and insurance': 'Automotive', 'Housekeeper/cook': 'Staffing & workforce solutions ', 'Regulatory Affairs- nutraceuticals ': 'Biopharma', 'Manufacturing : corporate admin support': 'Consumer Product Organization ', 'Household Services': 'Human services', 'Pharmacuticals': 'Medical/Pharmaceutical ', 'Actuarial': 'Law', 'Stay-at-home parent': 'Child and Yout Care', 'Commercial Building Material Distribution': 'Industrial Supply', 'Govtech Software as a Service': 'Software', 'Caregiver': 'Health care', 'Cancer research, not for profit': 'Academic/nonprofit research', 'Education/vocational': 'Career & Technical Training', 'IT MSP': 'IT Security', 'Payroll Software': 'Software', 'Geologist': 'Archaeologist', 'Low-Voltage Equipment': 'Aerospace', 'oceanography research': 'Scientist', 'Clinical & Translational Reserach': 'Translation', 'Architectural/Land Planning/Civil Engineering': 'Architect', 'Physical sciences': 'Biological Sciences', 'Data Entry': 'Information ', 'I work for Indeed.com': 'Freelance Journalism', \"Children's Book Wholesale\": 'Retail', 'State-level public transportation agency': 'Government ', 'Biotech/Food Safety': 'Biotech / Pharmaceutical Industry', 'e-learning': 'Instructional Design and Training', 'Wholesale/Distrbution': 'Manufacturing/Wholesale', 'Entrepreneur high net worth ': 'Accounting, Banking & Finance', 'University tech transfer (higher ed/marketing/writing)': 'Academic Publishing', 'Geospatial': 'Natural Resources', 'Industrial Cleaning & Non Hazardous Transport': 'Industrial Supply', 'Global Mobility': 'Corporate Travel Industry', 'Scientific R&D': 'Biological Sciences', 'Nuclear research': 'Research Science', \"Wherever I'm assigned via the union\": 'Labor Union', 'Immigration': 'International Development', 'awards and engraving': 'Art & Design', 'UX Research ': 'Research', 'Construction, HVAC ': 'Construction', 'Energy Supplier': 'Energy / renewables', 'Data/Institutional Research in Higher Education': 'Academic Science ', 'Cosmetology ': 'Apparel', 'Academia--cell and molecular biology': 'Biological Sciences', 'TIC (Testing, Inspection & Certification)': 'Career & Technical Training', 'School District Pre-K-12': 'Early Childhood Education', 'Science - QC lab': 'Analytical lab', 'commodities trading': 'E-Commerce', 'Interpretation': 'Law', 'Library science / part-time work/study': 'Higher education/Libraries', 'Winery regulatory compliance ': 'Compliance', 'Science and reasearch': 'Biological Sciences', 'Academic Press Production': 'Academic Publishing', 'Public affairs / PR ': 'Government Affairs/Lobbying', 'Construction / Stone industry': 'Construction, mining, manufacturing', 'Technical/Cybersecurity': 'IT Security', 'Gambling': 'Game Development', 'Child Care Resource and Referral Agency': 'Child and Yout Care', 'Forensics': 'Archaeologist', 'Educational assessment': 'Instructional Design and Training', 'AmeriCorps': 'Consumer Product Organization ', 'librarian--Contractor for NASA': 'Librarian', 'Private investigator at large firm': 'Engineering and Environmental Consulting', 'Executive Leadership Servis': 'Administration', 'Oil and Gas Safety Training': 'Energy - Oil and Gas', 'Human Capital Management': 'Human services', 'Shared office space': 'Family Office', 'In-House Marketing': 'Digital Marketing', 'Patent translation': 'Translation', 'Disability services ': 'Human services', 'Print / Mail': 'Print Distributor ', 'Drug development ': 'Medical/Pharmaceutical ', 'State DOT': 'Government ', 'Gyms': 'Fitness', 'Summer camp': 'Hospitality & Events', 'Database subscription services': 'Customer Service', 'CBD Manufacturing': 'Chemical Manufacturing', 'Brain research ': 'Research', 'Protective coatings': 'Chemical Manufacturing', 'Global Health Consulting': 'Political Consulting', 'Social networks': 'Research/Social Science'}\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained language model\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Function to reallocate categories that appear fewer than min_occurrence times\n",
    "def reallocate_with_AI(df, col_name, min_occurrence=3):\n",
    "    embeddings = model.encode(df[col_name].values)\n",
    "\n",
    "    value_counts = df[col_name].value_counts()\n",
    "    few_occurrences = value_counts[value_counts < min_occurrence].index\n",
    "    populated_categories = value_counts[value_counts >= min_occurrence].index\n",
    "    populated_embeddings = model.encode(populated_categories)\n",
    "\n",
    "    reallocation_dict = {}\n",
    "    \n",
    "    for industry in few_occurrences:\n",
    "        industry_embedding = model.encode([industry])\n",
    "        similarities = compute_cosine_similarity(populated_embeddings, industry_embedding)\n",
    "        closest_idx = similarities.argmax()\n",
    "        closest_category = populated_categories[closest_idx]\n",
    "        reallocation_dict[industry] = closest_category\n",
    "\n",
    "    df[col_name] = df[col_name].replace(reallocation_dict)\n",
    "    return df, reallocation_dict\n",
    "\n",
    "df_clean, reallocations = reallocate_with_AI(df_clean, 'Industry Category')\n",
    "\n",
    "print(reallocations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a20b54b-16f1-4f75-aaee-351a41f7b882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp                  27966\n",
      "Age Group                      7\n",
      "Industry                    1185\n",
      "Job Title                  14302\n",
      "Annual Salary               3654\n",
      "Additional Compensation      844\n",
      "Currency                      11\n",
      "Country                       97\n",
      "State                        134\n",
      "City                        4798\n",
      "Total Experience               8\n",
      "Field Experience               8\n",
      "Education Level                6\n",
      "Gender                         5\n",
      "Race                          50\n",
      "Industry Category            162\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Display the number of distinct values in each column\n",
    "distinct_values_count = df_clean.nunique()\n",
    "\n",
    "print(distinct_values_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd5a84b-de2c-48b8-936c-176e2f1b4347",
   "metadata": {},
   "source": [
    "### Column Currency\n",
    "Count how many rows have 'Other' as the currency and remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41308c64-0292-413a-adf8-67a920bacfaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with 'Other' currency: 158\n"
     ]
    }
   ],
   "source": [
    "other_currency_count = df_clean[df_clean['Currency'] == 'Other'].shape[0]\n",
    "print(f\"Number of rows with 'Other' currency: {other_currency_count}\")\n",
    "\n",
    "df_clean = df_clean[df_clean['Currency'] != 'Other']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f7117a-12c7-43f8-a815-14efaebedceb",
   "metadata": {},
   "source": [
    "### Converting all salary values to USD\n",
    "New column 'Annual Salary USD' added, using this rates:\n",
    "\n",
    "'USD': 1,\n",
    "'CAD': 1.38,\n",
    "'CHF': 0.87,\n",
    "'EUR': 0.92,\n",
    "'GBP': 0.77,\n",
    "'HKD': 7.77,\n",
    "'JPY': 149.49,\n",
    "'SEK': 10.53,\n",
    "'ZAR': 17.61,\n",
    "'AUD/NZD': 1.49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21cffdeb-1e9d-4f21-aad8-2b9fc250a6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Industry Category              Annual Salary Currency  Annual Salary USD\n",
      "1              Computing or Tech   54600         GBP      70909.090909     \n",
      "14                   Health care   32000         CAD      23188.405797     \n",
      "15            Telecommunications   24000         GBP      31168.831169     \n",
      "22  Nonprofit - legal department   63000         CAD      45652.173913     \n",
      "41             Digital Marketing   35000         GBP      45454.545455     \n",
      "48  Engineering or Manufacturing  120000         CAD      86956.521739     \n",
      "49                        Retail   97500         CAD      70652.173913     \n",
      "54                       Digital   52000         CAD      37681.159420     \n",
      "59                   Government    52000         GBP      67532.467532     \n",
      "61                   Government    79000         CAD      57246.376812     \n",
      "63               Adult education   75000         CAD      54347.826087     \n",
      "72             Health Insurance    45000         GBP      58441.558442     \n",
      "81               Adult education   36000         GBP      46753.246753     \n",
      "85  Nonprofit - legal department   60000         CAD      43478.260870     \n",
      "89             Computing or Tech  115000         CAD      83333.333333     \n"
     ]
    }
   ],
   "source": [
    "# Define conversion rates\n",
    "conversion_rates = {\n",
    "    'USD': 1,\n",
    "    'CAD': 1.38,\n",
    "    'CHF': 0.87,\n",
    "    'EUR': 0.92,\n",
    "    'GBP': 0.77,\n",
    "    'HKD': 7.77,\n",
    "    'JPY': 149.49,\n",
    "    'SEK': 10.53,\n",
    "    'ZAR': 17.61,\n",
    "    'AUD/NZD': 1.49\n",
    "}\n",
    "\n",
    "# Function to convert salary to USD\n",
    "def convert_to_usd(row):\n",
    "    currency = row['Currency']\n",
    "    annual_salary = row['Annual Salary']\n",
    "    \n",
    "    # Ensure the salary is valid (not null or zero)\n",
    "    if pd.isnull(annual_salary) or annual_salary <= 0:\n",
    "        return annual_salary  # Leave as is if salary is invalid\n",
    "    \n",
    "    # Convert salary if currency exists in the conversion rates\n",
    "    if currency in conversion_rates:\n",
    "        return annual_salary / conversion_rates[currency]\n",
    "    else:\n",
    "        return annual_salary  # No conversion if currency not in the dictionary\n",
    "\n",
    "# Apply the conversion to the 'Annual Salary' column\n",
    "df_clean['Annual Salary USD'] = df_clean.apply(convert_to_usd, axis=1)\n",
    "\n",
    "# Filter the DataFrame to show only rows where the currency is not 'USD'\n",
    "non_usd_df = df_clean[df_clean['Currency'] != 'USD']\n",
    "\n",
    "# Print the first 10 samples with currency not equal to 'USD'\n",
    "print(non_usd_df[['Industry Category', 'Annual Salary', 'Currency', 'Annual Salary USD']].head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d89b37-ffe4-4f2b-ac32-61d51c391e60",
   "metadata": {},
   "source": [
    "### Remove Outliers by Industry Category\n",
    "\n",
    "In the dataset, salary data is categorized by \"Industry Category.\" Some categories may have extreme values, or outliers, which can distort the analysis. Outliers are values that deviate significantly from the typical salary range for a particular industry.\n",
    "\n",
    "#### Why Outliers Matter:\n",
    "- **Skewed Analysis**: Outliers can cause mean salary values to be higher or lower than typical industry salaries.\n",
    "- **Misleading Insights**: Executive salaries or data errors can make it seem like certain industries pay more or less than they actually do for most jobs.\n",
    "\n",
    "#### Process in the Code:\n",
    "1. **Industry-Specific Filtering**: Outliers are detected within each \"Industry Category\" to ensure the filtering is appropriate for each industry’s salary range.\n",
    "2. **IQR Calculation**: For each industry, the upper bound is calculated based on the IQR, removing salaries beyond this threshold.\n",
    "3. **Results**: The cleaned dataset gives a more accurate representation of typical salaries in each industry, without the distortion of extreme values. Number of rows removed: 1021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f70657f3-2210-4caf-baae-eb2de6808030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows removed: 1021\n"
     ]
    }
   ],
   "source": [
    "def remove_upper_outliers_by_category(df, column, group_column):\n",
    "    \"\"\"\n",
    "    Removes rows where values in a column exceed the upper bound based on IQR within each group.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The original dataframe.\n",
    "        column (str): The name of the column for outlier detection (e.g., 'Annual Salary USD').\n",
    "        group_column (str): The column used for grouping (e.g., 'Industry Category').\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe with high outliers removed.\n",
    "    \"\"\"\n",
    "    def upper_bound_outliers(group):\n",
    "        Q3 = group[column].quantile(0.75)\n",
    "        IQR = Q3 - group[column].quantile(0.25)\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        return group[group[column] <= upper_bound]\n",
    "    \n",
    "    # Apply the upper bound filter function to each group\n",
    "    df_filtered = df.groupby(group_column, group_keys=False).apply(upper_bound_outliers)\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "# Apply the function to remove high outliers by 'Industry Category'\n",
    "df_clean_filtered = remove_upper_outliers_by_category(df_clean, 'Annual Salary USD', 'Industry Category')\n",
    "\n",
    "# Number of rows removed\n",
    "removed_rows = len(df_clean) - len(df_clean_filtered)\n",
    "print(f\"Number of rows removed: {removed_rows}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
